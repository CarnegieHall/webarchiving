# Web Archiving Primer

*DRAFT*

## What is Web Archiving?
"The process of selecting, capturing, saving and making accessible select content available online (e.g. websites)" - (p. 8 of [Web Archive Intro](https://www.slideshare.net/annaperricci/web-archiving-intro-circa-2015) by Anna Pericci).

Web content is very vulnerable to loss. Carnegie Hall is attempting to address this by building a web archiving strategy based with its [web archiving workplan](https://carnegiehall.github.io/webarchiving/workplan.html)

## What are Carnegie Hall's web archiving goals?

### Carnegie Hall Web Archiving Purpose Statement
The Carnegie Hall Archives aims to create a historic record of Carnegie Hall’s websites, available for internal reference and limited public use. The Archives will collaborate with each Department to capture a representative sample of their web resources. The preserved, curated materials will depict the content, style, and impact of the Hall’s online initiatives.

### Questions to consider

- How do we represent the web-based output from Carnegie Hall? Do we want evidence that X site existed at this time, contained this information as a snapshot? Or do we want to capture the experience of using the site?

- What needs are being met by web archive capture of sites?

- How is it possible to build in sustainability factors into creation? How can we support preservation by starting at the point of creation with responsible practices? 



### What are significant properties of websites?

What are the 'significant properties' or aspects of a website that need to be captured or maintained to appropriately represent what the site is? Does it matter if an aspect is missing in the captured/archived version? One way to assess this is to do a side-by-side comparison of the live site and capture. We can then ask ourselves if the site is "complete" based on what appears or doesn't appear in the capture. 

Some questions to consider when doing this comparison and thinking about signficant properties:
- Are images missing?
- Are media files not playing?
- Are text or design elements missing or inaccurate because the CSS is not captured? 
- Is navigation required, and to what extent? What links are broken?
- For sites with features that no longer function - how do you capture the existence and/or experience of the older, functioning site?

## Different types of captures

### Needs-based capture levels
Web Archiving can be done at different levels: 
1. Dynamic, curated capture to represent functionality and features
2. Low fidelity capture like high-level crawls (Wayback Machine) or even as simple as screenshots
3. Content harvest where all assets, media are captured for future management or reuse in updated capacity to align with current web goals 

![Screenshot of 1996 Carnegie Hall website](/ch1996.png)

_1996 iteration of the Carnegie Hall website (captured by and accessible in the Wayback Machine from Internet Archive)_

### Capture tools
#### Crawls
Crawler is software that indexes web content. Saved by a crawl: (also by AP)
- Code/info in web programming language (HTML, Flash)
- Some formatting (e.g., CSS)
- Text
- Images
- Some media files (embedded, but not the streamed ones)
- Documents, spreadsheets, presentations, data sets (XML, PDF, CSV)

Archive-It
Wget

#### Curated captures
Need project-by-project goal, as each site is unique. What is worthy of capture? What quality do we expect from each capture? 

Webrecorder

#### Web Archiving formats

WARC

#### Viewing captured sites

Wayback Machine
Webrecorder (UI and Player)
Emulation



