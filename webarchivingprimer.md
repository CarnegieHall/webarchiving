# Web Archiving Primer

*DRAFT*

## What is Web Archiving?
"The process of selecting, capturing, saving and making accessible select content available online (e.g. websites)" - (p. 8 of [Web Archive Intro](https://www.slideshare.net/annaperricci/web-archiving-intro-circa-2015) by Anna Pericci).

Web content is very vulnerable to loss! CH is attempting to address this by building a web archiving strategy based with its [web archiving workplan](https://carnegiehall.github.io/webarchiving/workplan.html)

Web Archiving can be done at different levels: 
- By harvesting content and republishing in updated capacity to align with current web goals 
- Through dynamic capture to represent functionality and features
- Screenshot, quick reference for low-fidelity capture 

## Different types of captures
Crawls vs curated captures
Difference between WARC capture vs content dump vs. text vs crawl
Saved by a crawl: (also by AP)
- Code/info in web programming language (HTML, Flash)
- Some formatting (e.g., CSS)
- Text
- Images
- Some media files (embedded, but not the streamed ones)
- Documents, spreadsheets, presentations, data sets (XML, PDF, CSV)

What is worthy of capture? What quality do we expect from each capture? 

Ex.: sites that no longer function; how to capture the older, functioning site 

Do we want evidence that X site existed at this time, contained this information? Or do we want to capture the experience of using the site? 

Need project-by-project goal, as each site is unique 

What is a web archive and common web archiving formats?

Crawler is softwarenis software that indexes web content

## What are web archiving goals?
What needs are being met by web archive capture of sites?
Representation of web-based output from an individual or organization
Build in sustainability factors into creation - start preservation process at point of creation 
